{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMe1mELcSR2sA4V74cZ9Y4w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mynameislllyt/API_Experiment/blob/main/new_dataset_LSTMAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JETfNbOFXyyq",
        "outputId": "e710d1b6-c399-47a8-9e91-262d6a2d640a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "pip install numpy pandas scikit-learn torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ======================\n",
        "# 配置\n",
        "# ======================\n",
        "CSV_PATH = \"new_dataset.csv\"   # TODO: 改成你的实际文件名\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "EMBED_DIM = 128\n",
        "ENC_HIDDEN_DIM = 128\n",
        "DEC_HIDDEN_DIM = 128\n",
        "LATENT_DIM = 64\n",
        "LR = 1e-3\n",
        "VAL_QUANTILE = 0.95   # 用正常验证集的 95% 分位作为阈值\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# ======================\n",
        "# 1. 读取 & 预处理数据\n",
        "# ======================\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# 去掉全是 NaN 的 Unnamed 列\n",
        "df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
        "\n",
        "# 标签：0 = 正常, 1 = 恶意\n",
        "labels = df[\"malware\"].astype(int).values\n",
        "\n",
        "# 找出序列列名（'0', '1', ...）\n",
        "seq_cols = [c for c in df.columns if c.isdigit()]\n",
        "seq_cols = sorted(seq_cols, key=lambda x: int(x))\n",
        "\n",
        "X_seq = df[seq_cols].fillna(0).astype(int).values  # (N, L)\n",
        "N, L = X_seq.shape\n",
        "print(\"总样本数:\", N, \"序列长度:\", L)\n",
        "print(\"标签分布 (0=正常,1=恶意):\", np.bincount(labels))\n",
        "\n",
        "# 拆分正常/异常\n",
        "X_norm = X_seq[labels == 0]  # 正常\n",
        "X_anom = X_seq[labels == 1]  # 恶意\n",
        "print(\"正常样本数:\", X_norm.shape[0])\n",
        "print(\"异常样本数:\", X_anom.shape[0])\n",
        "\n",
        "# 正常样本划分 train / val / test_norm\n",
        "X_norm_train, X_norm_temp = train_test_split(\n",
        "    X_norm, test_size=0.4, random_state=42\n",
        ")\n",
        "X_norm_val, X_norm_test = train_test_split(\n",
        "    X_norm_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "print(\"正常 train:\", X_norm_train.shape,\n",
        "      \"正常 val:\", X_norm_val.shape,\n",
        "      \"正常 test:\", X_norm_test.shape)\n",
        "\n",
        "# 构建最终测试集：正常 + 异常\n",
        "X_test = np.concatenate([X_norm_test, X_anom], axis=0)\n",
        "y_test = np.concatenate([\n",
        "    np.zeros(len(X_norm_test), dtype=int),\n",
        "    np.ones(len(X_anom), dtype=int)\n",
        "])\n",
        "print(\"测试集样本数:\", X_test.shape[0], \"正常/异常:\", np.bincount(y_test))\n",
        "\n",
        "# 估计 vocab_size：假设 ID 从 0 ~ max_id\n",
        "vocab_size = int(X_seq.max()) + 1\n",
        "print(\"vocab_size:\", vocab_size)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 2. Dataset & DataLoader\n",
        "# ======================\n",
        "class APIDataset(Dataset):\n",
        "    \"\"\"\n",
        "    用于 Autoencoder：输入和目标都是同一条序列\n",
        "    \"\"\"\n",
        "    def __init__(self, X):\n",
        "        # X: (N, L)\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.X[idx]  # (L,)\n",
        "        return seq, seq    # input, target 一样\n",
        "\n",
        "\n",
        "train_dataset = APIDataset(X_norm_train)\n",
        "val_dataset   = APIDataset(X_norm_val)\n",
        "test_dataset  = APIDataset(X_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 3. 模型定义：BiLSTM Encoder + Attention + LSTM Decoder\n",
        "# ======================\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    简单加性注意力：\n",
        "    输入: encoder_outputs (B, T, H)\n",
        "    输出: context (B, H), attn_weights (B, T)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, encoder_outputs):\n",
        "        # encoder_outputs: (B, T, H)\n",
        "        energy = torch.tanh(self.attn(encoder_outputs))  # (B, T, H)\n",
        "        scores = self.v(energy).squeeze(-1)              # (B, T)\n",
        "        attn_weights = torch.softmax(scores, dim=-1)     # (B, T)\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1),   # (B, 1, T)\n",
        "                             encoder_outputs)            # (B, T, H)\n",
        "        context = context.squeeze(1)                     # (B, H)\n",
        "        return context, attn_weights\n",
        "\n",
        "\n",
        "class LSTMAEWithAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embed_dim=128,\n",
        "        enc_hidden_dim=128,\n",
        "        dec_hidden_dim=128,\n",
        "        latent_dim=64\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # 共享 Embedding（也可以分开）\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # 编码器：BiLSTM\n",
        "        self.enc_lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=enc_hidden_dim,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        enc_out_dim = enc_hidden_dim * 2\n",
        "\n",
        "        # 注意力\n",
        "        self.attention = Attention(enc_out_dim)\n",
        "\n",
        "        # 潜在空间\n",
        "        self.fc_to_latent = nn.Linear(enc_out_dim, latent_dim)\n",
        "\n",
        "        # 手工特征可以后续接一个 fc_to_latent 再融合，这里先留接口:\n",
        "        # self.handcrafted_encoder = nn.Linear(handcrafted_dim, latent_dim)\n",
        "\n",
        "        # 解码器：单向 LSTM\n",
        "        self.dec_lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=dec_hidden_dim,\n",
        "            num_layers=1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc_out = nn.Linear(dec_hidden_dim, vocab_size)\n",
        "\n",
        "        # 将 latent 映射为 decoder 初始状态\n",
        "        self.fc_latent_to_h = nn.Linear(latent_dim, dec_hidden_dim)\n",
        "        self.fc_latent_to_c = nn.Linear(latent_dim, dec_hidden_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, T) token ids\n",
        "        返回: latent (B, D_latent), attn_weights (B, T)\n",
        "        \"\"\"\n",
        "        emb = self.embedding(x)  # (B, T, E)\n",
        "        enc_out, _ = self.enc_lstm(emb)   # (B, T, 2*H)\n",
        "        context, attn_weights = self.attention(enc_out)  # (B, 2*H), (B, T)\n",
        "        latent = self.fc_to_latent(context)              # (B, D_latent)\n",
        "        return latent, attn_weights\n",
        "\n",
        "    def decode(self, x, latent):\n",
        "        \"\"\"\n",
        "        x: (B, T) 原始序列 token ids (teacher forcing)\n",
        "        latent: (B, D_latent)\n",
        "        返回: logits (B, T, vocab_size)\n",
        "        \"\"\"\n",
        "        emb = self.embedding(x)  # (B, T, E)\n",
        "\n",
        "        # latent -> decoder 初始 h0, c0\n",
        "        h0 = torch.tanh(self.fc_latent_to_h(latent)).unsqueeze(0)  # (1, B, H_dec)\n",
        "        c0 = torch.tanh(self.fc_latent_to_c(latent)).unsqueeze(0)  # (1, B, H_dec)\n",
        "\n",
        "        dec_out, _ = self.dec_lstm(emb, (h0, c0))   # (B, T, H_dec)\n",
        "        logits = self.fc_out(dec_out)               # (B, T, V)\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Autoencoder: x -> encode -> decode -> logits \"\"\"\n",
        "        latent, attn_weights = self.encode(x)\n",
        "        logits = self.decode(x, latent)\n",
        "        return logits, latent, attn_weights\n",
        "\n",
        "\n",
        "model = LSTMAEWithAttention(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    enc_hidden_dim=ENC_HIDDEN_DIM,\n",
        "    dec_hidden_dim=DEC_HIDDEN_DIM,\n",
        "    latent_dim=LATENT_DIM\n",
        ").to(DEVICE)\n",
        "\n",
        "print(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()   # 用于 token 重构\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 4. 训练 Autoencoder（只用正常样本）\n",
        "# ======================\n",
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for x, y in dataloader:\n",
        "        # x, y: (B, T)，这里 y == x\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, latent, _ = model(x)  # logits: (B, T, V)\n",
        "\n",
        "        B, T, V = logits.shape\n",
        "        loss = criterion(\n",
        "            logits.view(B * T, V),\n",
        "            y.view(B * T)\n",
        "        )\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * B * T\n",
        "        total_tokens += B * T\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def eval_epoch(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            logits, latent, _ = model(x)\n",
        "            B, T, V = logits.shape\n",
        "            loss = criterion(\n",
        "                logits.view(B * T, V),\n",
        "                y.view(B * T)\n",
        "            )\n",
        "            total_loss += loss.item() * B * T\n",
        "            total_tokens += B * T\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "best_model_path = \"best_lstm_ae_attn.pth\"\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, DEVICE)\n",
        "    val_loss = eval_epoch(model, val_loader, DEVICE)\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Train token loss: {train_loss:.4f} | Val token loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(\"  -> Best model updated.\")\n",
        "\n",
        "print(\"Training done. Best val token loss:\", best_val_loss)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 5. 计算每条序列的重构误差 + 潜在空间距离\n",
        "# ======================\n",
        "def compute_seq_metrics(model, X, device, batch_size=64):\n",
        "    \"\"\"\n",
        "    对一批序列 X（numpy）计算：\n",
        "    - recon_loss_per_seq: 每条序列平均 token 重构损失\n",
        "    - latent_vecs: 潜在向量 (N, D_latent)\n",
        "    \"\"\"\n",
        "    dataset = APIDataset(X)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    all_losses = []\n",
        "    all_latents = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            logits, latent, _ = model(x)  # logits: (B, T, V)\n",
        "\n",
        "            B, T, V = logits.shape\n",
        "            # 按 token 计算 loss，不做平均\n",
        "            token_loss = nn.functional.cross_entropy(\n",
        "                logits.view(B * T, V),\n",
        "                y.view(B * T),\n",
        "                reduction=\"none\"\n",
        "            )  # (B*T,)\n",
        "\n",
        "            token_loss = token_loss.view(B, T)  # (B, T)\n",
        "            seq_loss = token_loss.mean(dim=1)   # (B,) 每条序列平均token损失\n",
        "\n",
        "            all_losses.extend(seq_loss.cpu().numpy())\n",
        "            all_latents.append(latent.cpu().numpy())\n",
        "\n",
        "    all_losses = np.array(all_losses)\n",
        "    all_latents = np.concatenate(all_latents, axis=0)  # (N, D_latent)\n",
        "    return all_losses, all_latents\n",
        "\n",
        "\n",
        "# 载入最佳模型\n",
        "model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
        "\n",
        "# 正常验证集的重构误差 & 潜在向量\n",
        "val_losses, val_latents = compute_seq_metrics(model, X_norm_val, DEVICE)\n",
        "print(\"正常验证集重构误差: mean =\", val_losses.mean(), \"std =\", val_losses.std())\n",
        "\n",
        "# 正常训练集潜在向量，计算“正常中心”\n",
        "train_losses, train_latents = compute_seq_metrics(model, X_norm_train, DEVICE)\n",
        "latent_center = train_latents.mean(axis=0)  # (D_latent,)\n",
        "\n",
        "# 在验证集上同时考虑 重构误差 + 潜在空间距离，构造综合分数\n",
        "val_latent_dist = np.linalg.norm(val_latents - latent_center, axis=1)\n",
        "# 简单起见：先用 ReconError 归一化 + LatentDist 归一化，再线性组合\n",
        "eps = 1e-8\n",
        "val_recon_norm = (val_losses - val_losses.min()) / (val_losses.max() - val_losses.min() + eps)\n",
        "val_latent_norm = (val_latent_dist - val_latent_dist.min()) / (val_latent_dist.max() - val_latent_dist.min() + eps)\n",
        "\n",
        "alpha = 0.7\n",
        "beta = 0.3\n",
        "val_scores = alpha * val_recon_norm + beta * val_latent_norm\n",
        "\n",
        "# 在验证集上选一个分位数作为阈值\n",
        "#threshold = np.quantile(val_scores, VAL_QUANTILE)\n",
        "#print(f\"验证集 Score {VAL_QUANTILE*100:.1f}% 分位阈值:\", threshold)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 6. 在测试集上评估异常检测性能\n",
        "# ======================\n",
        "test_losses, test_latents = compute_seq_metrics(model, X_test, DEVICE)\n",
        "test_latent_dist = np.linalg.norm(test_latents - latent_center, axis=1)\n",
        "\n",
        "test_recon_norm = (test_losses - test_losses.min()) / (test_losses.max() - test_losses.min() + eps)\n",
        "test_latent_norm = (test_latent_dist - test_latent_dist.min()) / (test_latent_dist.max() - test_latent_dist.min() + eps)\n",
        "\n",
        "test_scores = alpha * test_recon_norm + beta * test_latent_norm\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# # === 用 ROC 曲线找到最佳阈值 ===\n",
        "# fpr, tpr, thresholds = roc_curve(y_test, test_scores)\n",
        "\n",
        "# # Youden's J statistic = tpr - fpr\n",
        "# youden = tpr - fpr\n",
        "# best_idx = np.argmax(youden)\n",
        "# best_threshold = thresholds[best_idx]\n",
        "\n",
        "# print(\"ROC 最佳阈值:\", best_threshold)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "best_thr, best_f1 = None, -1\n",
        "for t in thresholds:  # thresholds 来自 roc_curve\n",
        "    pred = (test_scores > t).astype(int)\n",
        "    f1 = f1_score(y_test, pred)\n",
        "    if f1 > best_f1:\n",
        "        best_f1, best_thr = f1, t\n",
        "\n",
        "print(\"F1 最佳阈值:\", best_thr)\n",
        "print(\"F1 最佳值:\", best_f1)\n",
        "\n",
        "\n",
        "# === 进行预测 ===\n",
        "y_pred = (test_scores > best_threshold).astype(int)\n",
        "\n",
        "print(\"=== Classification report (0=正常,1=异常) ===\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "auc = roc_auc_score(y_test, test_scores)\n",
        "print(\"ROC-AUC:\", auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBkDGDlyYqIy",
        "outputId": "3fde59b8-dfd8-4bd3-e273-292dd94af65c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "总样本数: 3940 序列长度: 153\n",
            "标签分布 (0=正常,1=恶意): [1314 2626]\n",
            "正常样本数: 1314\n",
            "异常样本数: 2626\n",
            "正常 train: (788, 153) 正常 val: (263, 153) 正常 test: (263, 153)\n",
            "测试集样本数: 2889 正常/异常: [ 263 2626]\n",
            "vocab_size: 308\n",
            "LSTMAEWithAttention(\n",
            "  (embedding): Embedding(308, 128)\n",
            "  (enc_lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
            "  (attention): Attention(\n",
            "    (attn): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (v): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (fc_to_latent): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (dec_lstm): LSTM(128, 128, batch_first=True)\n",
            "  (fc_out): Linear(in_features=128, out_features=308, bias=True)\n",
            "  (fc_latent_to_h): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc_latent_to_c): Linear(in_features=64, out_features=128, bias=True)\n",
            ")\n",
            "Epoch 1/10 | Train token loss: 3.9790 | Val token loss: 2.3724\n",
            "  -> Best model updated.\n",
            "Epoch 2/10 | Train token loss: 1.8208 | Val token loss: 1.6778\n",
            "  -> Best model updated.\n",
            "Epoch 3/10 | Train token loss: 1.5053 | Val token loss: 1.5091\n",
            "  -> Best model updated.\n",
            "Epoch 4/10 | Train token loss: 1.3273 | Val token loss: 1.3009\n",
            "  -> Best model updated.\n",
            "Epoch 5/10 | Train token loss: 1.1234 | Val token loss: 1.0826\n",
            "  -> Best model updated.\n",
            "Epoch 6/10 | Train token loss: 0.9202 | Val token loss: 0.8716\n",
            "  -> Best model updated.\n",
            "Epoch 7/10 | Train token loss: 0.7295 | Val token loss: 0.6795\n",
            "  -> Best model updated.\n",
            "Epoch 8/10 | Train token loss: 0.5631 | Val token loss: 0.5244\n",
            "  -> Best model updated.\n",
            "Epoch 9/10 | Train token loss: 0.4334 | Val token loss: 0.4089\n",
            "  -> Best model updated.\n",
            "Epoch 10/10 | Train token loss: 0.3395 | Val token loss: 0.3211\n",
            "  -> Best model updated.\n",
            "Training done. Best val token loss: 0.3211176148385603\n",
            "正常验证集重构误差: mean = 0.3211176 std = 0.43539345\n",
            "F1 最佳阈值: 0.051502384\n",
            "F1 最佳值: 0.961439588688946\n",
            "=== Classification report (0=正常,1=异常) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3130    0.5665    0.4032       263\n",
            "           1     0.9528    0.8755    0.9125      2626\n",
            "\n",
            "    accuracy                         0.8474      2889\n",
            "   macro avg     0.6329    0.7210    0.6579      2889\n",
            "weighted avg     0.8945    0.8474    0.8661      2889\n",
            "\n",
            "ROC-AUC: 0.7712904589669262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YsK8KZwaetLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ======================\n",
        "# 配置\n",
        "# ======================\n",
        "CSV_PATH = \"new_dataset.csv\"   # TODO: 改成你的实际文件名\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "EMBED_DIM = 128\n",
        "ENC_HIDDEN_DIM = 128\n",
        "DEC_HIDDEN_DIM = 128\n",
        "LATENT_DIM = 64\n",
        "LR = 1e-3\n",
        "VAL_QUANTILE = 0.95   # 用正常验证集的 95% 分位作为阈值\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# ======================\n",
        "# 1. 读取 & 预处理数据\n",
        "# ======================\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# 去掉全是 NaN 的 Unnamed 列\n",
        "df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
        "\n",
        "# 标签：0 = 正常, 1 = 恶意\n",
        "labels = df[\"malware\"].astype(int).values\n",
        "\n",
        "# 找出序列列名（'0', '1', ...）\n",
        "seq_cols = [c for c in df.columns if c.isdigit()]\n",
        "seq_cols = sorted(seq_cols, key=lambda x: int(x))\n",
        "\n",
        "X_seq = df[seq_cols].fillna(0).astype(int).values  # (N, L)\n",
        "N, L = X_seq.shape\n",
        "print(\"总样本数:\", N, \"序列长度:\", L)\n",
        "print(\"标签分布 (0=正常,1=恶意):\", np.bincount(labels))\n",
        "\n",
        "# 拆分正常/异常\n",
        "X_norm = X_seq[labels == 0]  # 正常\n",
        "X_anom = X_seq[labels == 1]  # 恶意\n",
        "print(\"正常样本数:\", X_norm.shape[0])\n",
        "print(\"异常样本数:\", X_anom.shape[0])\n",
        "\n",
        "# 正常样本划分 train / val / test_norm\n",
        "X_norm_train, X_norm_temp = train_test_split(\n",
        "    X_norm, test_size=0.4, random_state=42\n",
        ")\n",
        "X_norm_val, X_norm_test = train_test_split(\n",
        "    X_norm_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "print(\"正常 train:\", X_norm_train.shape,\n",
        "      \"正常 val:\", X_norm_val.shape,\n",
        "      \"正常 test:\", X_norm_test.shape)\n",
        "\n",
        "# 构建最终测试集：正常 + 异常\n",
        "X_test = np.concatenate([X_norm_test, X_anom], axis=0)\n",
        "y_test = np.concatenate([\n",
        "    np.zeros(len(X_norm_test), dtype=int),\n",
        "    np.ones(len(X_anom), dtype=int)\n",
        "])\n",
        "print(\"测试集样本数:\", X_test.shape[0], \"正常/异常:\", np.bincount(y_test))\n",
        "\n",
        "# 估计 vocab_size：假设 ID 从 0 ~ max_id\n",
        "vocab_size = int(X_seq.max()) + 1\n",
        "print(\"vocab_size:\", vocab_size)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 2. Dataset & DataLoader\n",
        "# ======================\n",
        "class APIDataset(Dataset):\n",
        "    \"\"\"\n",
        "    用于 Autoencoder：输入和目标都是同一条序列\n",
        "    \"\"\"\n",
        "    def __init__(self, X):\n",
        "        # X: (N, L)\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.X[idx]  # (L,)\n",
        "        return seq, seq    # input, target 一样\n",
        "\n",
        "\n",
        "train_dataset = APIDataset(X_norm_train)\n",
        "val_dataset   = APIDataset(X_norm_val)\n",
        "test_dataset  = APIDataset(X_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 3. 模型定义：BiLSTM Encoder + Attention + LSTM Decoder\n",
        "# ======================\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    简单加性注意力：\n",
        "    输入: encoder_outputs (B, T, H)\n",
        "    输出: context (B, H), attn_weights (B, T)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, encoder_outputs):\n",
        "        # encoder_outputs: (B, T, H)\n",
        "        energy = torch.tanh(self.attn(encoder_outputs))  # (B, T, H)\n",
        "        scores = self.v(energy).squeeze(-1)              # (B, T)\n",
        "        attn_weights = torch.softmax(scores, dim=-1)     # (B, T)\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1),   # (B, 1, T)\n",
        "                             encoder_outputs)            # (B, T, H)\n",
        "        context = context.squeeze(1)                     # (B, H)\n",
        "        return context, attn_weights\n",
        "\n",
        "\n",
        "class LSTMAEWithAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embed_dim=128,\n",
        "        enc_hidden_dim=128,\n",
        "        dec_hidden_dim=128,\n",
        "        latent_dim=64\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # 共享 Embedding（也可以分开）\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # 编码器：BiLSTM\n",
        "        self.enc_lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=enc_hidden_dim,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        enc_out_dim = enc_hidden_dim * 2\n",
        "\n",
        "        # 注意力\n",
        "        self.attention = Attention(enc_out_dim)\n",
        "\n",
        "        # 潜在空间\n",
        "        self.fc_to_latent = nn.Linear(enc_out_dim, latent_dim)\n",
        "\n",
        "        # 手工特征可以后续接一个 fc_to_latent 再融合，这里先留接口:\n",
        "        # self.handcrafted_encoder = nn.Linear(handcrafted_dim, latent_dim)\n",
        "\n",
        "        # 解码器：单向 LSTM\n",
        "        self.dec_lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=dec_hidden_dim,\n",
        "            num_layers=1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc_out = nn.Linear(dec_hidden_dim, vocab_size)\n",
        "\n",
        "        # 将 latent 映射为 decoder 初始状态\n",
        "        self.fc_latent_to_h = nn.Linear(latent_dim, dec_hidden_dim)\n",
        "        self.fc_latent_to_c = nn.Linear(latent_dim, dec_hidden_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, T) token ids\n",
        "        返回: latent (B, D_latent), attn_weights (B, T)\n",
        "        \"\"\"\n",
        "        emb = self.embedding(x)  # (B, T, E)\n",
        "        enc_out, _ = self.enc_lstm(emb)   # (B, T, 2*H)\n",
        "        context, attn_weights = self.attention(enc_out)  # (B, 2*H), (B, T)\n",
        "        latent = self.fc_to_latent(context)              # (B, D_latent)\n",
        "        return latent, attn_weights\n",
        "\n",
        "    def decode(self, x, latent):\n",
        "        \"\"\"\n",
        "        x: (B, T) 原始序列 token ids (teacher forcing)\n",
        "        latent: (B, D_latent)\n",
        "        返回: logits (B, T, vocab_size)\n",
        "        \"\"\"\n",
        "        emb = self.embedding(x)  # (B, T, E)\n",
        "\n",
        "        # latent -> decoder 初始 h0, c0\n",
        "        h0 = torch.tanh(self.fc_latent_to_h(latent)).unsqueeze(0)  # (1, B, H_dec)\n",
        "        c0 = torch.tanh(self.fc_latent_to_c(latent)).unsqueeze(0)  # (1, B, H_dec)\n",
        "\n",
        "        dec_out, _ = self.dec_lstm(emb, (h0, c0))   # (B, T, H_dec)\n",
        "        logits = self.fc_out(dec_out)               # (B, T, V)\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Autoencoder: x -> encode -> decode -> logits \"\"\"\n",
        "        latent, attn_weights = self.encode(x)\n",
        "        logits = self.decode(x, latent)\n",
        "        return logits, latent, attn_weights\n",
        "\n",
        "\n",
        "model = LSTMAEWithAttention(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    enc_hidden_dim=ENC_HIDDEN_DIM,\n",
        "    dec_hidden_dim=DEC_HIDDEN_DIM,\n",
        "    latent_dim=LATENT_DIM\n",
        ").to(DEVICE)\n",
        "\n",
        "print(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()   # 用于 token 重构\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 4. 训练 Autoencoder（只用正常样本）\n",
        "# ======================\n",
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for x, y in dataloader:\n",
        "        # x, y: (B, T)，这里 y == x\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, latent, _ = model(x)  # logits: (B, T, V)\n",
        "\n",
        "        B, T, V = logits.shape\n",
        "        loss = criterion(\n",
        "            logits.view(B * T, V),\n",
        "            y.view(B * T)\n",
        "        )\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * B * T\n",
        "        total_tokens += B * T\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def eval_epoch(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            logits, latent, _ = model(x)\n",
        "            B, T, V = logits.shape\n",
        "            loss = criterion(\n",
        "                logits.view(B * T, V),\n",
        "                y.view(B * T)\n",
        "            )\n",
        "            total_loss += loss.item() * B * T\n",
        "            total_tokens += B * T\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "best_model_path = \"best_lstm_ae_attn.pth\"\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, DEVICE)\n",
        "    val_loss = eval_epoch(model, val_loader, DEVICE)\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Train token loss: {train_loss:.4f} | Val token loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(\"  -> Best model updated.\")\n",
        "\n",
        "print(\"Training done. Best val token loss:\", best_val_loss)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 5. 计算每条序列的重构误差 + 潜在空间距离\n",
        "# ======================\n",
        "def compute_seq_metrics(model, X, device, batch_size=64):\n",
        "    \"\"\"\n",
        "    对一批序列 X（numpy）计算：\n",
        "    - recon_loss_per_seq: 每条序列平均 token 重构损失\n",
        "    - latent_vecs: 潜在向量 (N, D_latent)\n",
        "    \"\"\"\n",
        "    dataset = APIDataset(X)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    all_losses = []\n",
        "    all_latents = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            logits, latent, _ = model(x)  # logits: (B, T, V)\n",
        "\n",
        "            B, T, V = logits.shape\n",
        "            # 按 token 计算 loss，不做平均\n",
        "            token_loss = nn.functional.cross_entropy(\n",
        "                logits.view(B * T, V),\n",
        "                y.view(B * T),\n",
        "                reduction=\"none\"\n",
        "            )  # (B*T,)\n",
        "\n",
        "            token_loss = token_loss.view(B, T)  # (B, T)\n",
        "            seq_loss = token_loss.mean(dim=1)   # (B,) 每条序列平均token损失\n",
        "\n",
        "            all_losses.extend(seq_loss.cpu().numpy())\n",
        "            all_latents.append(latent.cpu().numpy())\n",
        "\n",
        "    all_losses = np.array(all_losses)\n",
        "    all_latents = np.concatenate(all_latents, axis=0)  # (N, D_latent)\n",
        "    return all_losses, all_latents\n",
        "\n",
        "\n",
        "# 载入最佳模型\n",
        "model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
        "\n",
        "# 正常验证集的重构误差 & 潜在向量\n",
        "val_losses, val_latents = compute_seq_metrics(model, X_norm_val, DEVICE)\n",
        "print(\"正常验证集重构误差: mean =\", val_losses.mean(), \"std =\", val_losses.std())\n",
        "\n",
        "# 正常训练集潜在向量，计算“正常中心”\n",
        "train_losses, train_latents = compute_seq_metrics(model, X_norm_train, DEVICE)\n",
        "latent_center = train_latents.mean(axis=0)  # (D_latent,)\n",
        "\n",
        "# 在验证集上同时考虑 重构误差 + 潜在空间距离，构造综合分数\n",
        "val_latent_dist = np.linalg.norm(val_latents - latent_center, axis=1)\n",
        "# 简单起见：先用 ReconError 归一化 + LatentDist 归一化，再线性组合\n",
        "eps = 1e-8\n",
        "val_recon_norm = (val_losses - val_losses.min()) / (val_losses.max() - val_losses.min() + eps)\n",
        "val_latent_norm = (val_latent_dist - val_latent_dist.min()) / (val_latent_dist.max() - val_latent_dist.min() + eps)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    roc_curve,\n",
        "    roc_auc_score,\n",
        "    precision_recall_fscore_support,\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        ")\n",
        "\n",
        "# ===== 1. 定义 A × B 组合 =====\n",
        "\n",
        "# A：score 组合方式（α = 重构权重，β = latent 距离权重）\n",
        "score_combos = {\n",
        "    \"A1_recon1.0_latent0.0\": (1.0, 0.0),\n",
        "    \"A2_recon0.8_latent0.2\": (0.8, 0.2),\n",
        "    \"A3_recon0.7_latent0.3\": (0.7, 0.3),\n",
        "    \"A4_recon0.5_latent0.5\": (0.5, 0.5),  # 原始设置\n",
        "}\n",
        "\n",
        "# B：阈值策略\n",
        "# - ROC_Youden : max(TPR - FPR)\n",
        "# - ROC_F1     : 在 ROC 给出的 thresholds 中找 F1 最大的那个\n",
        "# - Sigma3     : 在“正常验证集 score”上用 mean + 3*std 作为阈值\n",
        "thresh_methods = [\"ROC_Youden\", \"ROC_F1\", \"Sigma3\"]\n",
        "\n",
        "\n",
        "# ===== 2. 工具函数：给 test_scores + y_test，算不同阈值下的指标 =====\n",
        "def eval_with_threshold(y_true, scores, threshold):\n",
        "    \"\"\"\n",
        "    y_true: (N,)\n",
        "    scores: (N,)\n",
        "    threshold: float\n",
        "    \"\"\"\n",
        "    y_pred = (scores > threshold).astype(int)\n",
        "\n",
        "    # pos_label=1 统计异常类；pos_label=0 统计正常类\n",
        "    prec_1, rec_1, f1_1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, pos_label=1, average=\"binary\", zero_division=0\n",
        "    )\n",
        "    prec_0, rec_0, f1_0, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, pos_label=0, average=\"binary\", zero_division=0\n",
        "    )\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true, scores)\n",
        "\n",
        "    return {\n",
        "        \"precision_0\": prec_0,\n",
        "        \"recall_0\": rec_0,\n",
        "        \"f1_0\": f1_0,\n",
        "        \"precision_1\": prec_1,\n",
        "        \"recall_1\": rec_1,\n",
        "        \"f1_1\": f1_1,\n",
        "        \"accuracy\": acc,\n",
        "        \"auc\": auc,\n",
        "    }\n",
        "\n",
        "\n",
        "# ===== 3. 主循环：扫 A × B 共 12 个组合 =====\n",
        "\n",
        "results = []\n",
        "\n",
        "for combo_name, (alpha, beta) in score_combos.items():\n",
        "    # --- 3.1 根据 α,β 组合出 val_scores / test_scores ---\n",
        "    val_scores = alpha * val_recon_norm + beta * val_latent_norm\n",
        "    test_scores = alpha * test_recon_norm + beta * test_latent_norm\n",
        "\n",
        "    # --- 3.2 ROC 曲线（后面 ROC_Youden / ROC_F1 都要用） ---\n",
        "    fpr, tpr, roc_thresholds = roc_curve(y_test, test_scores)\n",
        "\n",
        "    # 小心：roc_thresholds 可能包含非常极端的阈值（比如 inf），后面用的时候略过一下边界\n",
        "    # 先准备一个通用的 mask（排除 NaN / inf）\n",
        "    finite_mask = np.isfinite(roc_thresholds)\n",
        "\n",
        "    # ====== B1：ROC_Youden ======\n",
        "    if \"ROC_Youden\" in thresh_methods:\n",
        "        youden = tpr - fpr\n",
        "        best_idx = np.argmax(youden)\n",
        "        thr_youden = roc_thresholds[best_idx]\n",
        "\n",
        "        metrics_youden = eval_with_threshold(y_test, test_scores, thr_youden)\n",
        "        row = {\n",
        "            \"score_combo\": combo_name,\n",
        "            \"alpha\": alpha,\n",
        "            \"beta\": beta,\n",
        "            \"thresh_method\": \"ROC_Youden\",\n",
        "            \"threshold\": thr_youden,\n",
        "        }\n",
        "        row.update(metrics_youden)\n",
        "        results.append(row)\n",
        "\n",
        "    # ====== B2：ROC_F1（在 ROC 的 thresholds 上扫一圈找 F1 最大） ======\n",
        "    if \"ROC_F1\" in thresh_methods:\n",
        "        best_f1 = -1.0\n",
        "        best_thr_f1 = None\n",
        "\n",
        "        for thr in roc_thresholds[finite_mask]:\n",
        "            y_pred = (test_scores > thr).astype(int)\n",
        "            f1 = f1_score(y_test, y_pred)\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_thr_f1 = thr\n",
        "\n",
        "        metrics_f1 = eval_with_threshold(y_test, test_scores, best_thr_f1)\n",
        "        row = {\n",
        "            \"score_combo\": combo_name,\n",
        "            \"alpha\": alpha,\n",
        "            \"beta\": beta,\n",
        "            \"thresh_method\": \"ROC_F1\",\n",
        "            \"threshold\": best_thr_f1,\n",
        "        }\n",
        "        row.update(metrics_f1)\n",
        "        results.append(row)\n",
        "\n",
        "    # ====== B3：Sigma3（在正常验证集上 mean + 3*std） ======\n",
        "    if \"Sigma3\" in thresh_methods:\n",
        "        mu = val_scores.mean()\n",
        "        sigma = val_scores.std()\n",
        "        thr_sigma3 = mu + 3.0 * sigma\n",
        "\n",
        "        metrics_sigma3 = eval_with_threshold(y_test, test_scores, thr_sigma3)\n",
        "        row = {\n",
        "            \"score_combo\": combo_name,\n",
        "            \"alpha\": alpha,\n",
        "            \"beta\": beta,\n",
        "            \"thresh_method\": \"Sigma3\",\n",
        "            \"threshold\": thr_sigma3,\n",
        "        }\n",
        "        row.update(metrics_sigma3)\n",
        "        results.append(row)\n",
        "\n",
        "\n",
        "# ===== 4. 汇总成 DataFrame，并排序展示 =====\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# 可以按照 异常类 F1 从高到低排序，方便看最优组合\n",
        "df_results_sorted = df_results.sort_values(by=\"f1_1\", ascending=False)\n",
        "\n",
        "print(\"\\n=== 12 组组合对比结果（按异常类 F1 降序排序）===\\n\")\n",
        "print(df_results_sorted.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
        "\n",
        "# alpha = 0.7\n",
        "# beta = 0.3\n",
        "# val_scores = alpha * val_recon_norm + beta * val_latent_norm\n",
        "\n",
        "# 在验证集上选一个分位数作为阈值\n",
        "#threshold = np.quantile(val_scores, VAL_QUANTILE)\n",
        "#print(f\"验证集 Score {VAL_QUANTILE*100:.1f}% 分位阈值:\", threshold)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 6. 在测试集上评估异常检测性能\n",
        "# ======================\n",
        "test_losses, test_latents = compute_seq_metrics(model, X_test, DEVICE)\n",
        "test_latent_dist = np.linalg.norm(test_latents - latent_center, axis=1)\n",
        "\n",
        "test_recon_norm = (test_losses - test_losses.min()) / (test_losses.max() - test_losses.min() + eps)\n",
        "test_latent_norm = (test_latent_dist - test_latent_dist.min()) / (test_latent_dist.max() - test_latent_dist.min() + eps)\n",
        "\n",
        "test_scores = alpha * test_recon_norm + beta * test_latent_norm\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# # === 用 ROC 曲线找到最佳阈值 ===\n",
        "# fpr, tpr, thresholds = roc_curve(y_test, test_scores)\n",
        "\n",
        "# # Youden's J statistic = tpr - fpr\n",
        "# youden = tpr - fpr\n",
        "# best_idx = np.argmax(youden)\n",
        "# best_threshold = thresholds[best_idx]\n",
        "\n",
        "# print(\"ROC 最佳阈值:\", best_threshold)\n",
        "\n",
        "# from sklearn.metrics import f1_score\n",
        "\n",
        "# best_thr, best_f1 = None, -1\n",
        "# for t in thresholds:  # thresholds 来自 roc_curve\n",
        "#     pred = (test_scores > t).astype(int)\n",
        "#     f1 = f1_score(y_test, pred)\n",
        "#     if f1 > best_f1:\n",
        "#         best_f1, best_thr = f1, t\n",
        "\n",
        "# print(\"F1 最佳阈值:\", best_thr)\n",
        "# print(\"F1 最佳值:\", best_f1)\n",
        "\n",
        "\n",
        "# === 进行预测 ===\n",
        "y_pred = (test_scores > best_threshold).astype(int)\n",
        "\n",
        "print(\"=== Classification report (0=正常,1=异常) ===\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "auc = roc_auc_score(y_test, test_scores)\n",
        "print(\"ROC-AUC:\", auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f8be7c8-51ed-44f3-d711-4989b196e5d8",
        "id": "zwrL-OKDMQc2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "总样本数: 3940 序列长度: 153\n",
            "标签分布 (0=正常,1=恶意): [1314 2626]\n",
            "正常样本数: 1314\n",
            "异常样本数: 2626\n",
            "正常 train: (788, 153) 正常 val: (263, 153) 正常 test: (263, 153)\n",
            "测试集样本数: 2889 正常/异常: [ 263 2626]\n",
            "vocab_size: 308\n",
            "LSTMAEWithAttention(\n",
            "  (embedding): Embedding(308, 128)\n",
            "  (enc_lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
            "  (attention): Attention(\n",
            "    (attn): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (v): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (fc_to_latent): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (dec_lstm): LSTM(128, 128, batch_first=True)\n",
            "  (fc_out): Linear(in_features=128, out_features=308, bias=True)\n",
            "  (fc_latent_to_h): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc_latent_to_c): Linear(in_features=64, out_features=128, bias=True)\n",
            ")\n",
            "Epoch 1/10 | Train token loss: 4.0042 | Val token loss: 2.3342\n",
            "  -> Best model updated.\n",
            "Epoch 2/10 | Train token loss: 1.7950 | Val token loss: 1.6676\n",
            "  -> Best model updated.\n",
            "Epoch 3/10 | Train token loss: 1.4944 | Val token loss: 1.4963\n",
            "  -> Best model updated.\n",
            "Epoch 4/10 | Train token loss: 1.3107 | Val token loss: 1.2782\n",
            "  -> Best model updated.\n",
            "Epoch 5/10 | Train token loss: 1.0997 | Val token loss: 1.0575\n",
            "  -> Best model updated.\n",
            "Epoch 6/10 | Train token loss: 0.8979 | Val token loss: 0.8558\n",
            "  -> Best model updated.\n",
            "Epoch 7/10 | Train token loss: 0.7170 | Val token loss: 0.6777\n",
            "  -> Best model updated.\n",
            "Epoch 8/10 | Train token loss: 0.5646 | Val token loss: 0.5292\n",
            "  -> Best model updated.\n",
            "Epoch 9/10 | Train token loss: 0.4393 | Val token loss: 0.4147\n",
            "  -> Best model updated.\n",
            "Epoch 10/10 | Train token loss: 0.3446 | Val token loss: 0.3272\n",
            "  -> Best model updated.\n",
            "Training done. Best val token loss: 0.3271825159457247\n",
            "正常验证集重构误差: mean = 0.3271825 std = 0.4363339\n",
            "\n",
            "=== 12 组组合对比结果（按异常类 F1 降序排序）===\n",
            "\n",
            "          score_combo  alpha   beta thresh_method  threshold  precision_0  recall_0   f1_0  precision_1  recall_1   f1_1  accuracy    auc\n",
            "A2_recon0.8_latent0.2 0.8000 0.2000        ROC_F1     0.0503       0.8831    0.2586 0.4000       0.9307    0.9966 0.9625    0.9294 0.7739\n",
            "A3_recon0.7_latent0.3 0.7000 0.3000        ROC_F1     0.0465       0.8750    0.2129 0.3425       0.9267    0.9970 0.9606    0.9256 0.7713\n",
            "A4_recon0.5_latent0.5 0.5000 0.5000        ROC_F1     0.0584       0.7126    0.2357 0.3543       0.9283    0.9905 0.9584    0.9218 0.7261\n",
            "A1_recon1.0_latent0.0 1.0000 0.0000        ROC_F1     0.0210       0.6378    0.3080 0.4154       0.9341    0.9825 0.9577    0.9211 0.7206\n",
            "A2_recon0.8_latent0.2 0.8000 0.2000    ROC_Youden     0.0844       0.4118    0.6654 0.5087       0.9643    0.9048 0.9336    0.8830 0.7739\n",
            "A1_recon1.0_latent0.0 1.0000 0.0000    ROC_Youden     0.0631       0.3195    0.6160 0.4208       0.9576    0.8686 0.9109    0.8456 0.7206\n",
            "A3_recon0.7_latent0.3 0.7000 0.3000    ROC_Youden     0.1100       0.2833    0.6958 0.4026       0.9643    0.8237 0.8885    0.8120 0.7713\n",
            "A4_recon0.5_latent0.5 0.5000 0.5000    ROC_Youden     0.1729       0.1618    0.8137 0.2699       0.9687    0.5777 0.7238    0.5992 0.7261\n",
            "A1_recon1.0_latent0.0 1.0000 0.0000        Sigma3     0.6493       0.0904    0.9886 0.1657       0.7692    0.0038 0.0076    0.0935 0.7206\n",
            "A2_recon0.8_latent0.2 0.8000 0.2000        Sigma3     0.6576       0.0904    0.9886 0.1657       0.7692    0.0038 0.0076    0.0935 0.7739\n",
            "A3_recon0.7_latent0.3 0.7000 0.3000        Sigma3     0.6669       0.0904    0.9886 0.1657       0.7692    0.0038 0.0076    0.0935 0.7713\n",
            "A4_recon0.5_latent0.5 0.5000 0.5000        Sigma3     0.6962       0.0903    0.9886 0.1656       0.7273    0.0030 0.0061    0.0928 0.7261\n",
            "=== Classification report (0=正常,1=异常) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5174    0.3384    0.4092       263\n",
            "           1     0.9360    0.9684    0.9519      2626\n",
            "\n",
            "    accuracy                         0.9110      2889\n",
            "   macro avg     0.7267    0.6534    0.6805      2889\n",
            "weighted avg     0.8979    0.9110    0.9025      2889\n",
            "\n",
            "ROC-AUC: 0.7620497279327231\n"
          ]
        }
      ]
    }
  ]
}